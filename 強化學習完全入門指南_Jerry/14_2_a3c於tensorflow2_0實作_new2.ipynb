{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"14_2_a3c於tensorflow2_0實作_new2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyORxqF2rEOEH7FSmREvcF/7"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K3_MMYbgEfEP","executionInfo":{"status":"ok","timestamp":1615731015530,"user_tz":-480,"elapsed":12511,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"8c9e20e4-3bb9-4f1e-e4e1-b51d3261f3cb"},"source":["!apt-get install xvfb\r\n","!pip install pyvirtualdisplay"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following NEW packages will be installed:\n","  xvfb\n","0 upgraded, 1 newly installed, 0 to remove and 29 not upgraded.\n","Need to get 784 kB of archives.\n","After this operation, 2,270 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n","Fetched 784 kB in 2s (359 kB/s)\n","Selecting previously unselected package xvfb.\n","(Reading database ... 160975 files and directories currently installed.)\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Collecting pyvirtualdisplay\n","  Downloading https://files.pythonhosted.org/packages/19/88/7a198a5ee3baa3d547f5a49574cd8c3913b216f5276b690b028f89ffb325/PyVirtualDisplay-2.1-py3-none-any.whl\n","Collecting EasyProcess\n","  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n","Installing collected packages: EasyProcess, pyvirtualdisplay\n","Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":434},"id":"UnYtekzFEnmI","executionInfo":{"status":"ok","timestamp":1615731108622,"user_tz":-480,"elapsed":81399,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"97758a81-0b8d-4f3d-f65f-4205c9c7e9e5"},"source":["import os\r\n","\r\n","import gym\r\n","import time\r\n","import keras\r\n","import threading\r\n","import multiprocessing\r\n","\r\n","import numpy as np\r\n","import tensorflow as tf\r\n","print(tf.__version__)\r\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n","import matplotlib.pyplot as plt\r\n","\r\n","from PIL import Image\r\n","from queue import Queue\r\n","from keras import layers\r\n","from IPython import display\r\n","from pyvirtualdisplay import Display\r\n","\r\n","# 設定展示畫布大小\r\n","vir_display = Display(visible=0, size=(1400, 900))\r\n","vir_display.start()\r\n","\r\n","#建立紀錄函式\r\n","def record(episode,\r\n","           episode_reward,\r\n","           worker_idx,\r\n","           global_ep_reward,\r\n","           result_queue,\r\n","           total_loss,\r\n","           num_steps):\r\n","  if global_ep_reward == 0:\r\n","    global_ep_reward = episode_reward\r\n","  else:\r\n","    global_ep_reward = global_ep_reward * 0.99 + episode_reward * 0.01\r\n","  print(\r\n","      f\"迭代次數: {episode} | \"\r\n","      f\"平均移動獎勵: {int(global_ep_reward)} | \"\r\n","      f\"迭代獎勵: {int(episode_reward)} | \"\r\n","      f\"損失: {int(total_loss / float(num_steps) * 1000) / 1000} | \"\r\n","      f\"階段次數: {num_steps} | \"\r\n","      f\"智能體的代號: {worker_idx}\"\r\n","  )\r\n","  result_queue.put(global_ep_reward)\r\n","  return global_ep_reward\r\n","\r\n","#建立隨機的運作智能體\r\n","#每一個智能體都要呼叫gym的環境，包含環境、最大迭代數、預設的全域平均的獎勵、序列(先增加先取回)\r\n","class RandomAgent:\r\n","  def __init__(self, env_name, max_eps):\r\n","    self.env = gym.make(env_name)\r\n","    self.max_episodes = max_eps\r\n","    self.global_moving_average_reward = 0\r\n","    self.res_queue = Queue()\r\n","\r\n","  def run(self):\r\n","    #建立執行的初始化條件設定\r\n","    reward_avg = 0\r\n","    for episode in range(self.max_episodes):\r\n","      done = False\r\n","      self.env.reset()\r\n","      reward_sum = 0.0\r\n","      steps = 0\r\n","      while not done:\r\n","        # 隨機產生搖擺指令\r\n","        _, reward, done, _ = self.env.step(self.env.action_space.sample())\r\n","        steps += 1\r\n","      # 累計每一次的總分\r\n","        reward_sum += reward\r\n","      # 紀錄平均獎勵的參數，包含迭代、總分、序列等\r\n","      self.global_moving_average_reward = record(episode, \r\n","                                                 reward_sum, \r\n","                                                 0,\r\n","                                                 self.global_moving_average_reward,\r\n","                                                 self.res_queue, 0, steps)\r\n","\r\n","      reward_avg += reward_sum\r\n","    #將每一次的總獎勵除以最大的迭代次數，可以計算出平均的獎勵\r\n","    final_avg = reward_avg / float(self.max_episodes)\r\n","    print(\"Average score across {} episodes: {}\".format(self.max_episodes, final_avg))\r\n","    return final_avg\r\n","\r\n","\r\n","#建立負責動作獎懲與評估的ActorCritic\r\n","class ActorCriticModel(keras.Model):\r\n","  def __init__(self, state_size, action_size):\r\n","    super(ActorCriticModel, self).__init__()\r\n","    self.state_size = state_size\r\n","    self.action_size = action_size\r\n","    self.dense1 = layers.Dense(100, activation='relu')\r\n","    self.policy_logits = layers.Dense(action_size)\r\n","    self.dense2 = layers.Dense(100, activation='relu')\r\n","    self.values = layers.Dense(1)\r\n","\r\n","#ActorCriticModel定義好神經網路，call定義好神經網路運作邏輯\r\n","  def call(self, inputs):\r\n","    x = self.dense1(inputs)\r\n","    logits = self.policy_logits(x)\r\n","    v1 = self.dense2(inputs)\r\n","    values = self.values(v1)\r\n","    return logits, values\r\n","\r\n","#建立主要的Master智能體\r\n","class MasterAgent():\r\n","  def __init__(self):\r\n","    self.game_name = 'CartPole-v0'\r\n","    self.save_dir = save_dir\r\n","    if not os.path.exists(save_dir):\r\n","      os.makedirs(save_dir)\r\n","\r\n","    env = gym.make(self.game_name)\r\n","    self.state_size = env.observation_space.shape[0]\r\n","    self.action_size = env.action_space.n\r\n","    self.opt = keras.optimizers.Adam(lr)\r\n","    print(self.state_size, self.action_size)\r\n","\r\n","    self.global_model = ActorCriticModel(self.state_size, self.action_size)  # 建立全域網路\r\n","    self.global_model(np.random.random((1, self.state_size)).astype('float32'))\r\n","\r\n","  def train(self):\r\n","    #透過 Queue()建立序列\r\n","    res_queue = Queue()\r\n","    #建立智能體的參數\r\n","    workers = [Worker(self.state_size,\r\n","                      self.action_size,\r\n","                      self.global_model,\r\n","                      self.opt, res_queue,\r\n","                      i, game_name=self.game_name,\r\n","                      save_dir=self.save_dir) for i in range(multiprocessing.cpu_count())]\r\n","    #\r\n","    for i, worker in enumerate(workers):\r\n","      print(\"Starting worker {}\".format(i))\r\n","      worker.start()\r\n","\r\n","    moving_average_rewards = []  # 用於紀錄個時間點每局平均得分\r\n","    while True:\r\n","      reward = res_queue.get()\r\n","      if reward is not None:\r\n","        moving_average_rewards.append(reward)\r\n","      else:\r\n","        break\r\n","    [w.join() for w in workers]\r\n","\r\n","    plt.plot(moving_average_rewards)\r\n","    plt.ylabel('Moving Average Rewards')\r\n","    plt.xlabel('Step')\r\n","    plt.savefig(os.path.join(self.save_dir,\r\n","                             '{} Moving Average.png'.format(self.game_name)))\r\n","    plt.show()\r\n","\r\n","  def play(self):\r\n","    env = gym.make(self.game_name).unwrapped\r\n","    state = env.reset()\r\n","    model = self.global_model\r\n","    model_path = os.path.join(self.save_dir, 'model_{}.h5'.format(self.game_name))\r\n","    print('Loading model from: {}'.format(model_path))\r\n","    model.load_weights(model_path)\r\n","    done = False\r\n","    step_counter = 0\r\n","    reward_sum = 0\r\n","\r\n","    try:\r\n","      while not done:\r\n","        policy, value = model(state[None, :].astype('float32'))\r\n","        policy = tf.nn.softmax(policy)\r\n","        action = np.argmax(policy)\r\n","        state, reward, done, _ = env.step(action)\r\n","        reward_sum += reward\r\n","\r\n","        display.clear_output(wait=True)\r\n","        rgb_array = env.render(mode='rgb_array')\r\n","        img = Image.fromarray(rgb_array)\r\n","        display.display(img)\r\n","        print(\"階段:{}. 獎勵: {}, 動作: {}\".format(step_counter, reward_sum, action))\r\n","        time.sleep(0.015)\r\n","\r\n","        step_counter += 1\r\n","    except KeyboardInterrupt:\r\n","      print(\"Received Keyboard Interrupt. Shutting down.\")\r\n","    finally:\r\n","      env.close()\r\n","\r\n","#儲存每個智能體的表現\r\n","class Memory:\r\n","  def __init__(self):\r\n","    self.states = []\r\n","    self.actions = []\r\n","    self.rewards = []\r\n","\r\n","  def store(self, state, action, reward):\r\n","    self.states.append(state)\r\n","    self.actions.append(action)\r\n","    self.rewards.append(reward)\r\n","\r\n","  def clear(self):\r\n","    self.states = []\r\n","    self.actions = []\r\n","    self.rewards = []\r\n","\r\n","class Worker(threading.Thread):\r\n","  # 初始化當前局次\r\n","  global_episode = 0\r\n","  # 初始化當前每局平均得分\r\n","  global_moving_average_reward = 0\r\n","  best_score = 0\r\n","  save_lock = threading.Lock()\r\n","\r\n","  def __init__(self,\r\n","               state_size,\r\n","               action_size,\r\n","               global_model,\r\n","               opt,\r\n","               result_queue,\r\n","               idx,\r\n","               game_name='CartPole-v0',\r\n","               save_dir='tmp'):\r\n","    super(Worker, self).__init__()\r\n","    self.state_size = state_size\r\n","    self.action_size = action_size\r\n","    self.result_queue = result_queue\r\n","    self.global_model = global_model\r\n","    self.opt = opt\r\n","    self.local_model = ActorCriticModel(self.state_size, self.action_size)\r\n","    self.worker_idx = idx\r\n","    self.game_name = game_name\r\n","    self.env = gym.make(self.game_name).unwrapped\r\n","    self.save_dir = save_dir\r\n","    self.ep_loss = 0.0\r\n","\r\n","  def run(self):\r\n","    total_step = 1\r\n","    mem = Memory() #成果共享的儲存區\r\n","    while Worker.global_episode < max_eps:\r\n","      current_state = self.env.reset()\r\n","      mem.clear()\r\n","      ep_reward = 0.\r\n","      ep_steps = 0\r\n","      self.ep_loss = 0\r\n","\r\n","      time_count = 0\r\n","      done = False\r\n","      while not done:\r\n","        logits, _ = self.local_model(current_state[None, :].astype('float32'))\r\n","        probs = tf.nn.softmax(logits)\r\n","\r\n","        action = np.random.choice(self.action_size, p=probs.numpy()[0])\r\n","        new_state, reward, done, _ = self.env.step(action)\r\n","        if done:\r\n","          reward = -1\r\n","        ep_reward += reward\r\n","        mem.store(current_state, action, reward)\r\n","\r\n","        if time_count == update_freq or done:\r\n","          # 計算梯度修正以更新模型參數\r\n","          with tf.GradientTape() as tape:\r\n","            total_loss = self.compute_loss(done,\r\n","                                           new_state,\r\n","                                           mem,\r\n","                                           gamma)\r\n","          self.ep_loss += total_loss\r\n","          # 計算梯度修正\r\n","          grads = tape.gradient(total_loss, self.local_model.trainable_weights)\r\n","          # 為模型套用梯度修正\r\n","          self.opt.apply_gradients(zip(grads,\r\n","                                       self.global_model.trainable_weights))\r\n","          # 更新新修正的模型參數\r\n","          self.local_model.set_weights(self.global_model.get_weights())\r\n","\r\n","          mem.clear()\r\n","          time_count = 0\r\n","\r\n","          if done:  # 當局遊戲結束顯示得分\r\n","            Worker.global_moving_average_reward = \\\r\n","              record(Worker.global_episode, ep_reward, self.worker_idx,\r\n","                     Worker.global_moving_average_reward, self.result_queue,\r\n","                     self.ep_loss, ep_steps)\r\n","            # 獲取最好模型\r\n","            if ep_reward > Worker.best_score:\r\n","              with Worker.save_lock:\r\n","                print(\"儲存最佳的模型 {}, \"\r\n","                      \"迭代分數: {}\".format(self.save_dir, ep_reward))\r\n","                self.global_model.save_weights(\r\n","                    os.path.join(self.save_dir,\r\n","                                 'model_{}.h5'.format(self.game_name))\r\n","                )\r\n","                Worker.best_score = ep_reward\r\n","            Worker.global_episode += 1\r\n","        ep_steps += 1\r\n","\r\n","        time_count += 1\r\n","        current_state = new_state\r\n","        total_step += 1\r\n","    self.result_queue.put(None)\r\n","\r\n","  def compute_loss(self,\r\n","                   done,\r\n","                   new_state,\r\n","                   memory,\r\n","                   gamma=0.99):\r\n","    if done:\r\n","      reward_sum = 0.  # 遊戲結束，積分歸零\r\n","    else:\r\n","      reward_sum = self.local_model(new_state[None, :].astype('float32'))[-1].numpy()[0]\r\n","\r\n","    # 處理折扣獎勵\r\n","    discounted_rewards = []\r\n","    for reward in memory.rewards[::-1]:  \r\n","      reward_sum = reward + gamma * reward_sum\r\n","      discounted_rewards.append(reward_sum)\r\n","    discounted_rewards.reverse()\r\n","\r\n","    logits, values = self.local_model(np.vstack(memory.states).astype('float32'))\r\n","    # 計算advantage\r\n","    advantage = np.array(discounted_rewards)[:, None] - values\r\n","    # 得分損失\r\n","    value_loss = advantage ** 2\r\n","\r\n","    # 計算策略損失、模型驗證\r\n","    policy = tf.nn.softmax(logits)\r\n","    entropy = tf.nn.softmax_cross_entropy_with_logits(labels=policy, logits=logits)\r\n","\r\n","    policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=memory.actions, logits=logits)\r\n","\r\n","    policy_loss *= tf.stop_gradient(advantage)\r\n","    policy_loss -= 0.01 * entropy\r\n","    # 取得平均最低的Loss\r\n","    total_loss = tf.reduce_mean((0.5 * value_loss + policy_loss))  \r\n","    return total_loss\r\n","\r\n","# 單一智能體的成果測試\r\n","game_name, max_eps = 'CartPole-v0', 4000\r\n","random_agent = RandomAgent(game_name, max_eps)\r\n","random_agent.run()\r\n","\r\n","# 嘗試訓練A3C模型\r\n","save_dir = 'tmp/'\r\n","update_freq = 20\r\n","gamma = 0.99\r\n","lr = 0.001\r\n","max_eps = 300\r\n","\r\n","master = MasterAgent()\r\n","master.train()\r\n","\r\n","#用剛剛運算結果去跑\r\n","master = MasterAgent()\r\n","master.play()"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<PIL.Image.Image image mode=RGB size=600x400 at 0x7F05FF4D72D0>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["階段:623. 獎勵: 624.0, 動作: 0\n"],"name":"stdout"}]}]}