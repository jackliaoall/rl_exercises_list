{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8_2_以value_iteration方法實作過山車(mountain_car).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP1jC0yfj5RxNoIm7xm9zE7"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"UyXnHOrmE2CM","executionInfo":{"status":"ok","timestamp":1614992955347,"user_tz":-480,"elapsed":6457,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}}},"source":["def value_iteration(env):\r\n","    v = np.zeros(100)\r\n","    a = np.zeros(100)\r\n","    for i_episode in range(10): #Episode次數\r\n","        observation = env.reset()\r\n","        rewards = 0\r\n","        for t in range(100): #t時間做回饋\r\n","            action = env.action_space.sample() #隨機選擇動作（只有左右）\r\n","            observation, reward, done, info = env.step(action) #回饋獎勵、狀態\r\n","            new_state = observation\r\n","            q_sa = sum([1/2*(reward + new_state)])\r\n","            v[t] = max(q_sa)\r\n","            a[t] = action\r\n","            rewards += reward\r\n","        \r\n","            if done:\r\n","                break\r\n","    env.close()\r\n","    return a,v,rewards\r\n","\r\n","def extract_policy(env, iterations):\r\n","    p_temp = 0\r\n","    a_temp =[]\r\n","    p =[]\r\n","    for i in range(0,iterations):\r\n","        optimal_v = value_iteration(env)\r\n","        p = optimal_v[2]\r\n","        if p_temp > p:\r\n","            p_temp = p\r\n","            a_temp = optimal_v[0]\r\n","        else:\r\n","            pass\r\n","    return a_temp, p_temp\r\n","\r\n","import gym\r\n","import numpy as np\r\n","env = gym.make('MountainCar-v0')\r\n","a_temp, p_temp = extract_policy(env,100)\r\n","a_temp,p_temp\r\n","\r\n","import gym\r\n","import numpy as np\r\n","env = gym.make('MountainCar-v0')\r\n","a = a_temp.astype(int)\r\n","\r\n","for i_episode in range(10):\r\n","    observation = env.reset()\r\n","    rewards = 0\r\n","    for t in range(0,len(a)):\r\n","        #env.render()\r\n","        action = a[t]\r\n","        action = env.action_space.sample() #左(0)、不動(1)、右(2)\r\n","        observation, reward, done, info = env.step(action)\r\n","        #print(observation)\r\n","        rewards += reward\r\n","        #print(rewards)\r\n","        if done:\r\n","            print(\"迭代結束 {} 時間步長\".format(t+1),\"總獎勵\", rewards)\r\n","            break\r\n","env.close()"],"execution_count":4,"outputs":[]}]}