{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lesson5_DDPG.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOUUvuewm6GV+kXob+M3b/1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"uTn_PF4IiG7b"},"outputs":[],"source":["!pip uninstall -y parl  # 说明：AIStudio预装的parl版本太老，容易跟其他库产生兼容性冲突，建议先卸载\n","!pip uninstall -y pandas scikit-learn # 提示：在AIStudio中卸载这两个库再import parl可避免warning提示，不卸载也不影响parl的使用\n","\n","!pip install gym\n","!pip install paddlepaddle==1.6.3\n","!pip install parl==1.3.1\n","\n","!pip list | grep paddlepaddle\n","!pip list | grep parl"]},{"cell_type":"code","source":["import gym\n","import numpy as np\n","from copy import deepcopy\n","\n","import paddle.fluid as fluid\n","import parl\n","from parl import layers\n","from parl.utils import logger\n","\n","ACTOR_LR = 1e-3  # Actor网络的 learning rate\n","CRITIC_LR = 1e-3  # Critic网络的 learning rate\n","\n","GAMMA = 0.99      # reward 的衰减因子\n","TAU = 0.001       # 软更新的系数\n","MEMORY_SIZE = int(1e6)                  # 经验池大小\n","MEMORY_WARMUP_SIZE = MEMORY_SIZE // 20  # 预存一部分经验之后再开始训练\n","BATCH_SIZE = 128\n","REWARD_SCALE = 0.1   # reward 缩放系数\n","NOISE = 0.05         # 动作噪声方差\n","\n","TRAIN_EPISODE = 6000 # 训练的总episode数"],"metadata":{"id":"Iij1Qcy8iNA8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step4 搭建Model、Algorithm、Agent架构\n","\n","* Agent把产生的数据传给algorithm，algorithm根据model的模型结构计算出Loss，使用SGD或者其他优化器不断的优化，PARL这种架构可以很方便的应用在各类深度强化学习问题中。\n","\n","（1）Model\n","\n","* Model用来定义前向(Forward)网络，用户可以自由的定制自己的网络结构"],"metadata":{"id":"wRltag01iTvB"}},{"cell_type":"code","source":["class Model(parl.Model):\n","    def __init__(self, act_dim):\n","        self.actor_model = ActorModel(act_dim)\n","        self.critic_model = CriticModel()\n","\n","    def policy(self, obs):\n","        return self.actor_model.policy(obs)\n","\n","    def value(self, obs, act):\n","        return self.critic_model.value(obs, act)\n","\n","    def get_actor_params(self):\n","        return self.actor_model.parameters()\n","\n","\n","class ActorModel(parl.Model):\n","    def __init__(self, act_dim):\n","        hid_size = 100\n","\n","        self.fc1 = layers.fc(size=hid_size, act='relu')\n","        self.fc2 = layers.fc(size=act_dim, act='tanh')\n","\n","    def policy(self, obs):\n","        hid = self.fc1(obs)\n","        means = self.fc2(hid)\n","        return means\n","\n","\n","class CriticModel(parl.Model):\n","    def __init__(self):\n","        hid_size = 100\n","\n","        self.fc1 = layers.fc(size=hid_size, act='relu')\n","        self.fc2 = layers.fc(size=1, act=None)\n","\n","    def value(self, obs, act):\n","        concat = layers.concat([obs, act], axis=1)\n","        hid = self.fc1(concat)\n","        Q = self.fc2(hid)\n","        Q = layers.squeeze(Q, axes=[1])\n","        return Q"],"metadata":{"id":"NH7wu46xiW9z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["（2）Algorithm\n","\n","* Algorithm 定义了具体的算法来更新前向网络(Model)，也就是通过定义损失函数来更新Model，和算法相关的计算都放在algorithm中。"],"metadata":{"id":"nvA0dYKZiaXV"}},{"cell_type":"code","source":["class DDPG(parl.Algorithm):\n","    def __init__(self,\n","                 model,\n","                 gamma=None,\n","                 tau=None,\n","                 actor_lr=None,\n","                 critic_lr=None):\n","        \"\"\"  DDPG algorithm\n","        \n","        Args:\n","            model (parl.Model): actor and critic 的前向网络.\n","                                model 必须实现 get_actor_params() 方法.\n","            gamma (float): reward的衰减因子.\n","            tau (float): self.target_model 跟 self.model 同步参数 的 软更新参数\n","            actor_lr (float): actor 的学习率\n","            critic_lr (float): critic 的学习率\n","        \"\"\"\n","        assert isinstance(gamma, float)\n","        assert isinstance(tau, float)\n","        assert isinstance(actor_lr, float)\n","        assert isinstance(critic_lr, float)\n","        self.gamma = gamma\n","        self.tau = tau\n","        self.actor_lr = actor_lr\n","        self.critic_lr = critic_lr\n","\n","        self.model = model\n","        self.target_model = deepcopy(model)\n","\n","    def predict(self, obs):\n","        \"\"\" 使用 self.model 的 actor model 来预测动作\n","        \"\"\"\n","        return self.model.policy(obs)\n","\n","    def learn(self, obs, action, reward, next_obs, terminal):\n","        \"\"\" 用DDPG算法更新 actor 和 critic\n","        \"\"\"\n","        actor_cost = self._actor_learn(obs)\n","        critic_cost = self._critic_learn(obs, action, reward, next_obs,\n","                                         terminal)\n","        return actor_cost, critic_cost\n","\n","    def _actor_learn(self, obs):\n","        action = self.model.policy(obs)\n","        Q = self.model.value(obs, action)\n","        cost = layers.reduce_mean(-1.0 * Q)\n","        optimizer = fluid.optimizer.AdamOptimizer(self.actor_lr)\n","        optimizer.minimize(cost, parameter_list=self.model.get_actor_params())\n","        return cost\n","\n","    def _critic_learn(self, obs, action, reward, next_obs, terminal):\n","        next_action = self.target_model.policy(next_obs)\n","        next_Q = self.target_model.value(next_obs, next_action)\n","\n","        terminal = layers.cast(terminal, dtype='float32')\n","        target_Q = reward + (1.0 - terminal) * self.gamma * next_Q\n","        target_Q.stop_gradient = True\n","\n","        Q = self.model.value(obs, action)\n","        cost = layers.square_error_cost(Q, target_Q)\n","        cost = layers.reduce_mean(cost)\n","        optimizer = fluid.optimizer.AdamOptimizer(self.critic_lr)\n","        optimizer.minimize(cost)\n","        return cost\n","\n","    def sync_target(self, decay=None, share_vars_parallel_executor=None):\n","        \"\"\" self.target_model从self.model复制参数过来，可设置软更新参数\n","        \"\"\"\n","        if decay is None:\n","            decay = 1.0 - self.tau\n","        self.model.sync_weights_to(\n","            self.target_model,\n","            decay=decay,\n","            share_vars_parallel_executor=share_vars_parallel_executor)"],"metadata":{"id":"aTXuMEcjigfw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["（3）Agent\n","\n","* Agent负责算法与环境的交互，在交互过程中把生成的数据提供给Algorithm来更新模型(Model)，数据的预处理流程也一般定义在这里。"],"metadata":{"id":"6XQcromgiibg"}},{"cell_type":"code","source":["class Agent(parl.Agent):\n","    def __init__(self, algorithm, obs_dim, act_dim):\n","        assert isinstance(obs_dim, int)\n","        assert isinstance(act_dim, int)\n","        self.obs_dim = obs_dim\n","        self.act_dim = act_dim\n","        super(Agent, self).__init__(algorithm)\n","\n","        # 注意：最开始先同步self.model和self.target_model的参数.\n","        self.alg.sync_target(decay=0)\n","\n","    def build_program(self):\n","        self.pred_program = fluid.Program()\n","        self.learn_program = fluid.Program()\n","\n","        with fluid.program_guard(self.pred_program):\n","            obs = layers.data(\n","                name='obs', shape=[self.obs_dim], dtype='float32')\n","            self.pred_act = self.alg.predict(obs)\n","\n","        with fluid.program_guard(self.learn_program):\n","            obs = layers.data(\n","                name='obs', shape=[self.obs_dim], dtype='float32')\n","            act = layers.data(\n","                name='act', shape=[self.act_dim], dtype='float32')\n","            reward = layers.data(name='reward', shape=[], dtype='float32')\n","            next_obs = layers.data(\n","                name='next_obs', shape=[self.obs_dim], dtype='float32')\n","            terminal = layers.data(name='terminal', shape=[], dtype='bool')\n","            _, self.critic_cost = self.alg.learn(obs, act, reward, next_obs,\n","                                                 terminal)\n","\n","    def predict(self, obs):\n","        obs = np.expand_dims(obs, axis=0)\n","        act = self.fluid_executor.run(\n","            self.pred_program, feed={'obs': obs},\n","            fetch_list=[self.pred_act])[0]\n","        act = np.squeeze(act)\n","        return act\n","\n","    def learn(self, obs, act, reward, next_obs, terminal):\n","        feed = {\n","            'obs': obs,\n","            'act': act,\n","            'reward': reward,\n","            'next_obs': next_obs,\n","            'terminal': terminal\n","        }\n","        critic_cost = self.fluid_executor.run(\n","            self.learn_program, feed=feed, fetch_list=[self.critic_cost])[0]\n","        self.alg.sync_target()\n","        return critic_cost"],"metadata":{"id":"iX_6WmBuimr-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#env.py\n","\n","连续控制版本的CartPole环境\n","\n","该环境代码与算法无关，可忽略不看"],"metadata":{"id":"HG-CKSczixWL"}},{"cell_type":"code","source":["# env.py\n","# Continuous version of Cartpole\n","\n","import math\n","import gym\n","from gym import spaces\n","from gym.utils import seeding\n","import numpy as np\n","\n","\n","class ContinuousCartPoleEnv(gym.Env):\n","    metadata = {\n","        'render.modes': ['human', 'rgb_array'],\n","        'video.frames_per_second': 50\n","    }\n","\n","    def __init__(self):\n","        self.gravity = 9.8\n","        self.masscart = 1.0\n","        self.masspole = 0.1\n","        self.total_mass = (self.masspole + self.masscart)\n","        self.length = 0.5  # actually half the pole's length\n","        self.polemass_length = (self.masspole * self.length)\n","        self.force_mag = 30.0\n","        self.tau = 0.02  # seconds between state updates\n","        self.min_action = -1.0\n","        self.max_action = 1.0\n","\n","        # Angle at which to fail the episode\n","        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n","        self.x_threshold = 2.4\n","\n","        # Angle limit set to 2 * theta_threshold_radians so failing observation\n","        # is still within bounds\n","        high = np.array([\n","            self.x_threshold * 2,\n","            np.finfo(np.float32).max,\n","            self.theta_threshold_radians * 2,\n","            np.finfo(np.float32).max])\n","\n","        self.action_space = spaces.Box(\n","            low=self.min_action,\n","            high=self.max_action,\n","            shape=(1,)\n","        )\n","        self.observation_space = spaces.Box(-high, high)\n","\n","        self.seed()\n","        self.viewer = None\n","        self.state = None\n","\n","        self.steps_beyond_done = None\n","\n","    def seed(self, seed=None):\n","        self.np_random, seed = seeding.np_random(seed)\n","        return [seed]\n","\n","    def stepPhysics(self, force):\n","        x, x_dot, theta, theta_dot = self.state\n","        costheta = math.cos(theta)\n","        sintheta = math.sin(theta)\n","        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass\n","        thetaacc = (self.gravity * sintheta - costheta * temp) / \\\n","            (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass))\n","        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n","        x = x + self.tau * x_dot\n","        x_dot = x_dot + self.tau * xacc\n","        theta = theta + self.tau * theta_dot\n","        theta_dot = theta_dot + self.tau * thetaacc\n","        return (x, x_dot, theta, theta_dot)\n","\n","    def step(self, action):\n","        action = np.expand_dims(action, 0)\n","        assert self.action_space.contains(action), \\\n","            \"%r (%s) invalid\" % (action, type(action))\n","        # Cast action to float to strip np trappings\n","        force = self.force_mag * float(action)\n","        self.state = self.stepPhysics(force)\n","        x, x_dot, theta, theta_dot = self.state\n","        done = x < -self.x_threshold \\\n","            or x > self.x_threshold \\\n","            or theta < -self.theta_threshold_radians \\\n","            or theta > self.theta_threshold_radians\n","        done = bool(done)\n","\n","        if not done:\n","            reward = 1.0\n","        elif self.steps_beyond_done is None:\n","            # Pole just fell!\n","            self.steps_beyond_done = 0\n","            reward = 1.0\n","        else:\n","            if self.steps_beyond_done == 0:\n","                gym.logger.warn(\"\"\"\n","You are calling 'step()' even though this environment has already returned\n","done = True. You should always call 'reset()' once you receive 'done = True'\n","Any further steps are undefined behavior.\n","                \"\"\")\n","            self.steps_beyond_done += 1\n","            reward = 0.0\n","\n","        return np.array(self.state), reward, done, {}\n","\n","    def reset(self):\n","        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n","        self.steps_beyond_done = None\n","        return np.array(self.state)\n","\n","    def render(self, mode='human'):\n","        screen_width = 600\n","        screen_height = 400\n","\n","        world_width = self.x_threshold * 2\n","        scale = screen_width /world_width\n","        carty = 100  # TOP OF CART\n","        polewidth = 10.0\n","        polelen = scale * 1.0\n","        cartwidth = 50.0\n","        cartheight = 30.0\n","\n","        if self.viewer is None:\n","            from gym.envs.classic_control import rendering\n","            self.viewer = rendering.Viewer(screen_width, screen_height)\n","            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n","            axleoffset = cartheight / 4.0\n","            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n","            self.carttrans = rendering.Transform()\n","            cart.add_attr(self.carttrans)\n","            self.viewer.add_geom(cart)\n","            l, r, t, b = -polewidth / 2, polewidth / 2, polelen-polewidth / 2, -polewidth / 2\n","            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n","            pole.set_color(.8, .6, .4)\n","            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n","            pole.add_attr(self.poletrans)\n","            pole.add_attr(self.carttrans)\n","            self.viewer.add_geom(pole)\n","            self.axle = rendering.make_circle(polewidth / 2)\n","            self.axle.add_attr(self.poletrans)\n","            self.axle.add_attr(self.carttrans)\n","            self.axle.set_color(.5, .5, .8)\n","            self.viewer.add_geom(self.axle)\n","            self.track = rendering.Line((0, carty), (screen_width, carty))\n","            self.track.set_color(0, 0, 0)\n","            self.viewer.add_geom(self.track)\n","\n","        if self.state is None:\n","            return None\n","\n","        x = self.state\n","        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART\n","        self.carttrans.set_translation(cartx, carty)\n","        self.poletrans.set_rotation(-x[2])\n","\n","        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))\n","\n","    def close(self):\n","        if self.viewer:\n","            self.viewer.close()"],"metadata":{"id":"h9y7fS5iiz7t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#replay_memory.py\n","\n","经验池 ReplayMemory\n","\n","* 与DQN的replay_mamory.py代码一致"],"metadata":{"id":"iPSyCVOhi30b"}},{"cell_type":"code","source":["# replay_memory.py\n","import random\n","import collections\n","import numpy as np\n","\n","\n","class ReplayMemory(object):\n","    def __init__(self, max_size):\n","        self.buffer = collections.deque(maxlen=max_size)\n","\n","    def append(self, exp):\n","        self.buffer.append(exp)\n","\n","    def sample(self, batch_size):\n","        mini_batch = random.sample(self.buffer, batch_size)\n","        obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = [], [], [], [], []\n","\n","        for experience in mini_batch:\n","            s, a, r, s_p, done = experience\n","            obs_batch.append(s)\n","            action_batch.append(a)\n","            reward_batch.append(r)\n","            next_obs_batch.append(s_p)\n","            done_batch.append(done)\n","\n","        return np.array(obs_batch).astype('float32'), \\\n","            np.array(action_batch).astype('float32'), np.array(reward_batch).astype('float32'),\\\n","            np.array(next_obs_batch).astype('float32'), np.array(done_batch).astype('float32')\n","\n","    def __len__(self):\n","        return len(self.buffer)"],"metadata":{"id":"XpKQzTLZi9rX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Step5 Training && Test（训练&&测试）"],"metadata":{"id":"ZTSzWHJHi-NO"}},{"cell_type":"code","source":["def run_episode(agent, env, rpm):\n","    obs = env.reset()\n","    total_reward = 0\n","    steps = 0\n","    while True:\n","        steps += 1\n","        batch_obs = np.expand_dims(obs, axis=0)\n","        action = agent.predict(batch_obs.astype('float32'))\n","\n","        # 增加探索扰动, 输出限制在 [-1.0, 1.0] 范围内\n","        action = np.clip(np.random.normal(action, NOISE), -1.0, 1.0)\n","\n","        next_obs, reward, done, info = env.step(action)\n","\n","        action = [action]  # 方便存入replaymemory\n","        rpm.append((obs, action, REWARD_SCALE * reward, next_obs, done))\n","\n","        if len(rpm) > MEMORY_WARMUP_SIZE and (steps % 5) == 0:\n","            (batch_obs, batch_action, batch_reward, batch_next_obs,\n","             batch_done) = rpm.sample(BATCH_SIZE)\n","            agent.learn(batch_obs, batch_action, batch_reward, batch_next_obs,\n","                        batch_done)\n","\n","        obs = next_obs\n","        total_reward += reward\n","\n","        if done or steps >= 200:\n","            break\n","    return total_reward\n","\n","\n","def evaluate(env, agent, render=False):\n","    eval_reward = []\n","    for i in range(5):\n","        obs = env.reset()\n","        total_reward = 0\n","        steps = 0\n","        while True:\n","            batch_obs = np.expand_dims(obs, axis=0)\n","            action = agent.predict(batch_obs.astype('float32'))\n","            action = np.clip(action, -1.0, 1.0)\n","\n","            steps += 1\n","            next_obs, reward, done, info = env.step(action)\n","\n","            obs = next_obs\n","            total_reward += reward\n","\n","            if render:\n","                env.render()\n","            if done or steps >= 200:\n","                break\n","        eval_reward.append(total_reward)\n","    return np.mean(eval_reward)"],"metadata":{"id":"fFV0BD9Si-Vq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Step6 创建环境和Agent，创建经验池，启动训练，保存模型"],"metadata":{"id":"dLu5LNwFjDRe"}},{"cell_type":"code","source":["# 创建环境\n","env = ContinuousCartPoleEnv()\n","\n","obs_dim = env.observation_space.shape[0]\n","act_dim = env.action_space.shape[0]\n","\n","# 使用PARL框架创建agent\n","model = Model(act_dim)\n","algorithm = DDPG(\n","    model, gamma=GAMMA, tau=TAU, actor_lr=ACTOR_LR, critic_lr=CRITIC_LR)\n","agent = Agent(algorithm, obs_dim, act_dim)\n","\n","# 创建经验池\n","rpm = ReplayMemory(MEMORY_SIZE)\n","# 往经验池中预存数据\n","while len(rpm) < MEMORY_WARMUP_SIZE:\n","    run_episode(agent, env, rpm)\n","\n","episode = 0\n","while episode < TRAIN_EPISODE:\n","    for i in range(50):\n","        total_reward = run_episode(agent, env, rpm)\n","        episode += 1\n","\n","    eval_reward = evaluate(env, agent, render=False)\n","    logger.info('episode:{}    test_reward:{}'.format(\n","        episode, eval_reward))"],"metadata":{"id":"V8QbusVYjDcQ"},"execution_count":null,"outputs":[]}]}