{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"12_2_deep_q_learning於tensorflow實作.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyONh335gfgSvWyc73XFRLqo"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WQDc_qdKCZ1B","executionInfo":{"status":"ok","timestamp":1615730456599,"user_tz":-480,"elapsed":3334,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"d4a7831c-f846-4854-ca43-e03e8f6732ef"},"source":["import random\r\n","from collections import deque\r\n","import gym\r\n","import numpy as np\r\n","import tensorflow as tf\r\n","from tensorflow import keras\r\n","from tensorflow.keras import layers\r\n","\r\n","env = gym.make('CartPole-v1')\r\n","state_size = env.observation_space.shape[0]\r\n","action_size = env.action_space.n\r\n","\r\n","Iteration = 10\r\n","gamma = 0.8 #折扣率\r\n","epsilon = 1  #Epsilon\r\n","learning_rate = 0.001\r\n","batch_size = 16\r\n","\r\n","model = tf.keras.Sequential()\r\n","model.add(layers.Dense(16, input_dim=state_size, activation='relu'))\r\n","model.add(layers.Dense(24, activation='relu'))\r\n","model.add(layers.Dense(action_size, activation='linear')) #依照動作種類輸出\r\n","model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), loss='mse', metrics=['mse'])\r\n","\r\n","def agent(state):\r\n","    if np.random.rand() <= epsilon:\r\n","      action = np.random.choice(action_size)\r\n","    else:\r\n","      q = learning_rate*(reward + gamma * np.amax(model.predict(next_state)[0]))\r\n","      if q > 0.5:\r\n","        action = 1\r\n","      else:\r\n","        action = 0\r\n","    return action\r\n","\r\n","for i in range(Iteration):\r\n","    state = env.reset()\r\n","    state = np.reshape(state, [1, state_size])\r\n","    rewards = 0\r\n","    for t in range(100):\r\n","        action = agent(state)\r\n","        next_state, reward, done, info = env.step(action)\r\n","        rewards += reward\r\n","        next_state = np.reshape(next_state, [1, state_size])\r\n","        state = next_state\r\n","        if done:\r\n","          print('Episode結束 {} 次, 總獎勵 {}'.format(t+1, rewards))\r\n","          break"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Episode結束 32 次, 總獎勵 32.0\n","Episode結束 19 次, 總獎勵 19.0\n","Episode結束 15 次, 總獎勵 15.0\n","Episode結束 11 次, 總獎勵 11.0\n","Episode結束 24 次, 總獎勵 24.0\n","Episode結束 20 次, 總獎勵 20.0\n","Episode結束 28 次, 總獎勵 28.0\n","Episode結束 17 次, 總獎勵 17.0\n","Episode結束 30 次, 總獎勵 30.0\n","Episode結束 14 次, 總獎勵 14.0\n"],"name":"stdout"}]}]}