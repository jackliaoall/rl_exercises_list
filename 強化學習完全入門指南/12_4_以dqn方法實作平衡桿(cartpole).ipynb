{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"12_4_以dqn方法實作平衡桿(cartpole).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO0X99U4XR+2LOosKEx4Q6b"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tVzH5XGYC4m7","executionInfo":{"status":"ok","timestamp":1615730967326,"user_tz":-480,"elapsed":369706,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"6150c227-cc06-4a1d-de84-16e26c1bfa5d"},"source":["import random\r\n","from collections import deque\r\n","import gym\r\n","import numpy as np\r\n","import tensorflow as tf\r\n","from tensorflow import keras\r\n","from tensorflow.keras import layers\r\n","\r\n","archive = deque(maxlen=100)\r\n","\r\n","env = gym.make('CartPole-v1')\r\n","state_size = env.observation_space.shape[0]\r\n","action_size = env.action_space.n\r\n","\r\n","Iteration = 10\r\n","gamma = 0.8 #折扣率\r\n","epsilon = 1  #Epsilon\r\n","learning_rate = 0.001\r\n","batch_size = 16\r\n","\r\n","model = tf.keras.Sequential()\r\n","model.add(layers.Dense(16, input_dim=state_size, activation='relu'))\r\n","model.add(layers.Dense(24, activation='relu'))\r\n","model.add(layers.Dense(action_size, activation='linear')) #依照動作種類輸出\r\n","model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), loss='mse', metrics=['mse'])\r\n","\r\n","def agent(state):\r\n","    if np.random.rand() <= epsilon:\r\n","      action = np.random.choice(action_size)\r\n","    else:\r\n","      action = model.predict(state)\r\n","    return action\r\n","\r\n","for i in range(Iteration):\r\n","    state = env.reset()\r\n","    state = np.reshape(state, [1, state_size])\r\n","    rewards = 0\r\n","    for t in range(100):\r\n","        action = agent(state)\r\n","        next_state, reward, done, info = env.step(action)\r\n","        archive.append((state, action, reward, next_state, done))\r\n","        rewards += reward\r\n","        next_state = np.reshape(next_state, [1, state_size])\r\n","        state = next_state\r\n","        if done:\r\n","          print('Episode結束 {} 次, 總獎勵 {}'.format(t+1, rewards))\r\n","          break\r\n","        if len(archive) > batch_size:\r\n","          batch = random.sample(archive, batch_size)\r\n","          for state, action, reward, next_state, done in batch:\r\n","            if done != True:\r\n","              next_state = np.reshape(next_state, [1, state_size])\r\n","              q = learning_rate*(reward + gamma * np.amax(model.predict(next_state)[0]))\r\n","            target_all = model.predict(state)\r\n","            target_all[0][action] = q\r\n","            model.fit(state, target_all, epochs=1, verbose=0)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Episode結束 12 次, 總獎勵 12.0\n","Episode結束 15 次, 總獎勵 15.0\n","Episode結束 14 次, 總獎勵 14.0\n","Episode結束 17 次, 總獎勵 17.0\n","Episode結束 17 次, 總獎勵 17.0\n","Episode結束 40 次, 總獎勵 40.0\n","Episode結束 14 次, 總獎勵 14.0\n","Episode結束 13 次, 總獎勵 13.0\n","Episode結束 17 次, 總獎勵 17.0\n","Episode結束 41 次, 總獎勵 41.0\n"],"name":"stdout"}]}]}