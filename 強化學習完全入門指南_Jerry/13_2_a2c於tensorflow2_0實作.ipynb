{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"13_2_a2c於tensorflow2_0實作.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNwQX8ErKzdV0j++xhFkg+H"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zLE94q_GEB-c","executionInfo":{"status":"ok","timestamp":1615730978978,"user_tz":-480,"elapsed":60920,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"710d16aa-b6e0-4203-fd53-cd8dc2017eaa"},"source":["import gym\r\n","import numpy as np\r\n","import tensorflow as tf\r\n","import matplotlib.pyplot as plt\r\n","import tensorflow.keras.layers as kl\r\n","import tensorflow.keras.losses as kls\r\n","import tensorflow.keras.optimizers as ko\r\n","\r\n","class A2CAgent:\r\n","  def __init__(self, model, lr=7e-3, gamma=0.99, value_c=0.5, entropy_c=1e-4):\r\n","    self.gamma = gamma\r\n","    self.value_c = value_c\r\n","    self.entropy_c = entropy_c\r\n","    self.model = model\r\n","    self.model.compile(\r\n","      optimizer=ko.RMSprop(lr=lr),\r\n","      # 定義策略的Loss與價值函數的Loss\r\n","      loss=[self._logits_loss, self._value_loss])\r\n","\r\n","  def train(self, env, batch_sz=64, updates=100):\r\n","    actions = np.empty((batch_sz,), dtype=np.int32) #儲存批次動作數據\r\n","    rewards, dones, values = np.empty((3, batch_sz)) #儲存批次獎勵、是否結束等數據\r\n","    observations = np.empty((batch_sz,) + env.observation_space.shape) #儲存批次觀測值\r\n","    # 進入訓練的迭代\r\n","    ep_rewards = [0.0]\r\n","    next_obs = env.reset()\r\n","    for update in range(updates):\r\n","      for step in range(batch_sz):\r\n","        observations[step] = next_obs.copy()\r\n","        actions[step], values[step] = self.model.action_value(next_obs[None, :]) #取得動作、價值\r\n","        next_obs, rewards[step], dones[step], _ = env.step(actions[step]) #取得動作之後的回饋值\r\n","\r\n","        ep_rewards[-1] += rewards[step]\r\n","        if dones[step]:\r\n","          ep_rewards.append(0.0)\r\n","          next_obs = env.reset()\r\n","          print(\"迭代: %03d, 獎勵: %03d\" % (len(ep_rewards) - 1, ep_rewards[-2]))\r\n","\r\n","      _, next_value = self.model.action_value(next_obs[None, :]) #動作函數\r\n","      returns, advs = self._returns_advantages(rewards, dones, values, next_value) #優勢函數\r\n","      acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1) #整合動作與動作優勢函數\r\n","      losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\r\n","\r\n","    return ep_rewards\r\n","\r\n","  def test(self, env, render=False):  #測試\r\n","    obs, done, ep_reward = env.reset(), False, 0\r\n","    while not done:\r\n","      action, _ = self.model.action_value(obs[None, :])\r\n","      obs, reward, done, _ = env.step(action)\r\n","      ep_reward += reward\r\n","      if render:\r\n","        env.render()\r\n","    return ep_reward\r\n","\r\n","  def _returns_advantages(self, rewards, dones, values, next_value): #優勢函數\r\n","    # 評論\r\n","    returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\r\n","    # 回傳折扣率之後的獎勵\r\n","    for t in reversed(range(rewards.shape[0])):\r\n","      returns[t] = rewards[t] + self.gamma * returns[t + 1] * (1 - dones[t])\r\n","    returns = returns[:-1]\r\n","    advantages = returns - values\r\n","    return returns, advantages\r\n","\r\n","  def _value_loss(self, returns, value):\r\n","    # 計算Value Loss\r\n","    return self.value_c * kls.mean_squared_error(returns, value)\r\n","\r\n","  def _logits_loss(self, actions_and_advantages, logits): \r\n","    # 計算策略的Loss\r\n","    actions, advantages = tf.split(actions_and_advantages, 2, axis=-1)\r\n","    weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\r\n","    actions = tf.cast(actions, tf.int32)\r\n","    policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\r\n","    probs = tf.nn.softmax(logits)\r\n","    entropy_loss = kls.categorical_crossentropy(probs, probs)\r\n","    return policy_loss - self.entropy_c * entropy_loss\r\n","\r\n","\r\n","class ProbabilityDistribution(tf.keras.Model): #機率模型\r\n","  def call(self, logits, **kwargs):\r\n","    return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\r\n","\r\n","class Model(tf.keras.Model):#建立簡單的神經網路\r\n","  def __init__(self, num_actions):\r\n","    super().__init__('mlp_policy')\r\n","    self.hidden1 = kl.Dense(128, activation='relu')\r\n","    self.hidden2 = kl.Dense(128, activation='relu')\r\n","    self.value = kl.Dense(1, name='value')\r\n","    self.logits = kl.Dense(num_actions, name='policy_logits')\r\n","    self.dist = ProbabilityDistribution()\r\n","\r\n","  def call(self, inputs, **kwargs):\r\n","    x = tf.convert_to_tensor(inputs) \r\n","    # 隱藏層做分開\r\n","    hidden_logs = self.hidden1(x)\r\n","    hidden_vals = self.hidden2(x)\r\n","    return self.logits(hidden_logs), self.value(hidden_vals)\r\n","\r\n","  def action_value(self, obs):\r\n","    logits, value = self.predict_on_batch(obs)\r\n","    action = self.dist.predict_on_batch(logits)\r\n","    return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)\r\n","\r\n","env = gym.make('CartPole-v0')\r\n","model = Model(num_actions=env.action_space.n)\r\n","\r\n","agent = A2CAgent(model)\r\n","rewards_history = agent.train(env)\r\n","print(\"總迭代獲得的獎勵: %d out of 200\" % agent.test(env))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["迭代: 001, 獎勵: 029\n","迭代: 002, 獎勵: 015\n","迭代: 003, 獎勵: 017\n","迭代: 004, 獎勵: 024\n","迭代: 005, 獎勵: 017\n","迭代: 006, 獎勵: 040\n","迭代: 007, 獎勵: 034\n","迭代: 008, 獎勵: 066\n","迭代: 009, 獎勵: 052\n","迭代: 010, 獎勵: 052\n","迭代: 011, 獎勵: 012\n","迭代: 012, 獎勵: 075\n","迭代: 013, 獎勵: 029\n","迭代: 014, 獎勵: 015\n","迭代: 015, 獎勵: 046\n","迭代: 016, 獎勵: 020\n","迭代: 017, 獎勵: 047\n","迭代: 018, 獎勵: 039\n","迭代: 019, 獎勵: 015\n","迭代: 020, 獎勵: 018\n","迭代: 021, 獎勵: 045\n","迭代: 022, 獎勵: 039\n","迭代: 023, 獎勵: 081\n","迭代: 024, 獎勵: 065\n","迭代: 025, 獎勵: 097\n","迭代: 026, 獎勵: 130\n","迭代: 027, 獎勵: 033\n","迭代: 028, 獎勵: 031\n","迭代: 029, 獎勵: 033\n","迭代: 030, 獎勵: 036\n","迭代: 031, 獎勵: 082\n","迭代: 032, 獎勵: 041\n","迭代: 033, 獎勵: 060\n","迭代: 034, 獎勵: 033\n","迭代: 035, 獎勵: 152\n","迭代: 036, 獎勵: 082\n","迭代: 037, 獎勵: 060\n","迭代: 038, 獎勵: 061\n","迭代: 039, 獎勵: 165\n","迭代: 040, 獎勵: 166\n","迭代: 041, 獎勵: 110\n","迭代: 042, 獎勵: 094\n","迭代: 043, 獎勵: 200\n","迭代: 044, 獎勵: 200\n","迭代: 045, 獎勵: 200\n","迭代: 046, 獎勵: 194\n","迭代: 047, 獎勵: 101\n","迭代: 048, 獎勵: 200\n","迭代: 049, 獎勵: 139\n","迭代: 050, 獎勵: 104\n","迭代: 051, 獎勵: 032\n","迭代: 052, 獎勵: 200\n","迭代: 053, 獎勵: 200\n","迭代: 054, 獎勵: 200\n","迭代: 055, 獎勵: 181\n","迭代: 056, 獎勵: 080\n","迭代: 057, 獎勵: 108\n","迭代: 058, 獎勵: 035\n","迭代: 059, 獎勵: 188\n","迭代: 060, 獎勵: 161\n","迭代: 061, 獎勵: 129\n","迭代: 062, 獎勵: 084\n","迭代: 063, 獎勵: 180\n","迭代: 064, 獎勵: 200\n","迭代: 065, 獎勵: 200\n","迭代: 066, 獎勵: 131\n","迭代: 067, 獎勵: 036\n","迭代: 068, 獎勵: 176\n","迭代: 069, 獎勵: 156\n","總迭代獲得的獎勵: 200 out of 200\n"],"name":"stdout"}]}]}