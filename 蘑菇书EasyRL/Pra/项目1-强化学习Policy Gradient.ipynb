{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 强化学习-Policy Gradient\n",
    "## 强化学习\n",
    "- 强化学习（Reinforcement learning，简称RL）是机器学习中的一个领域，区别与监督学习和无监督学习，强调如何基于环境而行动，以取得最大化的预期利益。\n",
    "- 基本操作步骤：智能体`agent`在环境`environment`中学习，根据环境的状态`state`（或观测到的`observation`），执行动作`action`，并根据环境的反馈`reward`（奖励）来指导更好的动作。\n",
    "\n",
    "比如本项目的Cart pole小游戏中，`agent`就是动图中的杆子，杆子有向左向右两种`action`。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/17c106ff3b724082a9405eca76adaa712a40d46936bf4f59887f3c41bbdf976f)\n",
    "\n",
    "## Policy Gradient简介\n",
    "* 在强化学习中，有两大类方法，一种基于值（`Value-based`），一种基于策略（`Policy-based`）\n",
    "    * `Value-based`的算法的典型代表为`Q-learning`和`SARSA`，将`Q`函数优化到最优，再根据`Q`函数取最优策略。\n",
    "    * `Policy-based`的算法的典型代表为`Policy Gradient`，直接优化策略函数。\n",
    "* 采用神经网络拟合策略函数，需计算策略梯度用于优化策略网络。\n",
    "    * 优化的目标是在策略`π(s,a)`的期望回报：所有的轨迹获得的回报`R`与对应的轨迹发生概率`p`的加权和，当N足够大时，可通过采样N个Episode求平均的方式近似表达。\n",
    "    \n",
    "    ![](https://ai-studio-static-online.cdn.bcebos.com/eb184ddf8dcc4dc3b528a105f8d8e3ea6487d4905bc04cdebd7725f2d6a2752f)\n",
    "    \n",
    "    * 优化目标对参数`θ`求导后得到策略梯度：\n",
    "    \n",
    "    ![](https://ai-studio-static-online.cdn.bcebos.com/326d8abe040347cea25e4c0be3e09015e85cb818a02c445483381540ab1d238c)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 安装依赖\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting pygame\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/32/ce/c40213f819148f7afaa39102fb68be5037fe0fc88e45d8fc5aa8bac64492/pygame-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/21.8 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pygame\n",
      "Successfully installed pygame-2.1.2\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: gym in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (0.12.1)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gym) (1.16.0)\n",
      "Requirement already satisfied: requests>=2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gym) (2.22.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gym) (1.3.0)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gym) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gym) (1.16.4)\n",
      "Requirement already satisfied: future in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pyglet>=1.2.0->gym) (0.18.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.0->gym) (1.25.6)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.0->gym) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.0->gym) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.0->gym) (2019.9.11)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting atari_py\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7a/ad/bf0b26d4aa571e393619bd4d77e6ccb45f39a23d87f9a67080e02fa7b831/atari_py-0.2.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from atari_py) (1.16.4)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from atari_py) (1.16.0)\n",
      "Installing collected packages: atari_py\n",
      "Successfully installed atari_py-0.2.9\n"
     ]
    }
   ],
   "source": [
    "!pip install pygame\n",
    "!pip install gym\n",
    "!pip install atari_py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 导入相关库\n",
    "\n",
    "本部分代码使用PaddlePaddle2.0版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import numpy as np\n",
    "import paddle.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型Model\n",
    "\n",
    "这里的模型可以根据自己的需求选择不同的神经网络组建。\n",
    "\n",
    "`PolicyGradient`用来定义前向(`Forward`)网络，可以自由的定制自己的网络结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PolicyGradient(nn.Layer):\n",
    "    def __init__(self, act_dim):\n",
    "        super(PolicyGradient, self).__init__()\n",
    "        act_dim = act_dim\n",
    "        hid1_size = act_dim * 10\n",
    "\n",
    "        self.linear1 = nn.Linear(in_features=4, out_features=hid1_size)\n",
    "        self.linear2 = nn.Linear(in_features=hid1_size, out_features=act_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        out = self.linear1(obs)\n",
    "        out = paddle.tanh(out)\n",
    "        out = self.linear2(out)\n",
    "        out = F.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 智能体Agent的学习函数\n",
    "这里包括模型探索与模型训练两个部分\n",
    "\n",
    "`Agent`负责算法与环境的交互，在交互过程中把生成的数据提供给`Algorithm`来更新模型(`Model`)，数据的预处理流程也一般定义在这里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample(obs, MODEL):\n",
    "    global ACTION_DIM\n",
    "    obs = np.expand_dims(obs, axis=0)\n",
    "    obs = paddle.to_tensor(obs, dtype='float32')\n",
    "    act = MODEL(obs)\n",
    "    act_prob = np.squeeze(act, axis=0)\n",
    "    act = np.random.choice(range(ACTION_DIM), p=act_prob.numpy())\n",
    "    return act\n",
    "\n",
    "\n",
    "def learn(obs, action, reward, MODEL):\n",
    "    obs = np.array(obs).astype('float32')\n",
    "    obs = paddle.to_tensor(obs)\n",
    "    act_prob = MODEL(obs)\n",
    "    action = paddle.to_tensor(action.astype('int32'))\n",
    "    log_prob = paddle.sum(-1.0 * paddle.log(act_prob) * F.one_hot(action, act_prob.shape[1]), axis=1)\n",
    "    reward = paddle.to_tensor(reward.astype('float32'))\n",
    "    cost = log_prob * reward\n",
    "    cost = paddle.sum(cost)\n",
    "\n",
    "    opt = paddle.optimizer.Adam(learning_rate=LEARNING_RATE,\n",
    "                                parameters=MODEL.parameters())  # 优化器(动态图)\n",
    "    cost.backward()\n",
    "    opt.step()\n",
    "    opt.clear_grad()\n",
    "    return cost.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型梯度更新算法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_train(env, MODEL):\n",
    "    MODEL.train()\n",
    "    obs_list, action_list, total_reward = [], [], []\n",
    "    obs = env.reset()\n",
    "\n",
    "    while True:\n",
    "        # 获取随机动作和执行游戏\n",
    "        obs_list.append(obs)\n",
    "        action = sample(obs, MODEL) # 采样动作\n",
    "        action_list.append(action)\n",
    "        \n",
    "        obs, reward, isOver, info = env.step(action)\n",
    "        total_reward.append(reward)\n",
    "        \n",
    "        # 结束游戏\n",
    "        if isOver:\n",
    "            break\n",
    "    return obs_list, action_list, total_reward\n",
    "\n",
    "\n",
    "def evaluate(model, env, render=False):\n",
    "    model.eval()\n",
    "    eval_reward = []\n",
    "    for i in range(5):\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            obs = np.expand_dims(obs, axis=0)\n",
    "            obs = paddle.to_tensor(obs, dtype='float32')\n",
    "            action = model(obs)\n",
    "            action = np.argmax(action.numpy())\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "        eval_reward.append(episode_reward)\n",
    "    return np.mean(eval_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练函数与验证函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "设置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001  # 学习率大小\n",
    "\n",
    "OBS_DIM = None\n",
    "ACTION_DIM = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 根据一个episode的每个step的reward列表，计算每一个Step的Gt\n",
    "def calc_reward_to_go(reward_list, gamma=1.0):\n",
    "    for i in range(len(reward_list) - 2, -1, -1):\n",
    "        # G_t = r_t + γ·r_t+1 + ... = r_t + γ·G_t+1\n",
    "        reward_list[i] += gamma * reward_list[i + 1]  # Gt\n",
    "    return np.array(reward_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Episode 0, Reward Sum 14.0.\n",
      "Episode 10, Reward Sum 25.0.\n",
      "Episode 20, Reward Sum 12.0.\n",
      "Episode 30, Reward Sum 10.0.\n",
      "Episode 40, Reward Sum 22.0.\n",
      "Episode 50, Reward Sum 15.0.\n",
      "Episode 60, Reward Sum 15.0.\n",
      "Episode 70, Reward Sum 16.0.\n",
      "Episode 80, Reward Sum 60.0.\n",
      "Episode 90, Reward Sum 15.0.\n",
      "Test reward: 33.6\n",
      "Episode 100, Reward Sum 24.0.\n",
      "Episode 110, Reward Sum 18.0.\n",
      "Episode 120, Reward Sum 18.0.\n",
      "Episode 130, Reward Sum 30.0.\n",
      "Episode 140, Reward Sum 42.0.\n",
      "Episode 150, Reward Sum 35.0.\n",
      "Episode 160, Reward Sum 33.0.\n",
      "Episode 170, Reward Sum 13.0.\n",
      "Episode 180, Reward Sum 14.0.\n",
      "Episode 190, Reward Sum 37.0.\n",
      "Test reward: 50.6\n",
      "Episode 200, Reward Sum 31.0.\n",
      "Episode 210, Reward Sum 24.0.\n",
      "Episode 220, Reward Sum 18.0.\n",
      "Episode 230, Reward Sum 20.0.\n",
      "Episode 240, Reward Sum 48.0.\n",
      "Episode 250, Reward Sum 46.0.\n",
      "Episode 260, Reward Sum 58.0.\n",
      "Episode 270, Reward Sum 57.0.\n",
      "Episode 280, Reward Sum 17.0.\n",
      "Episode 290, Reward Sum 35.0.\n",
      "Test reward: 55.0\n",
      "Episode 300, Reward Sum 21.0.\n",
      "Episode 310, Reward Sum 31.0.\n",
      "Episode 320, Reward Sum 54.0.\n",
      "Episode 330, Reward Sum 45.0.\n",
      "Episode 340, Reward Sum 57.0.\n",
      "Episode 350, Reward Sum 52.0.\n",
      "Episode 360, Reward Sum 28.0.\n",
      "Episode 370, Reward Sum 47.0.\n",
      "Episode 380, Reward Sum 62.0.\n",
      "Episode 390, Reward Sum 61.0.\n",
      "Test reward: 58.6\n",
      "Episode 400, Reward Sum 25.0.\n",
      "Episode 410, Reward Sum 48.0.\n",
      "Episode 420, Reward Sum 43.0.\n",
      "Episode 430, Reward Sum 35.0.\n",
      "Episode 440, Reward Sum 16.0.\n",
      "Episode 450, Reward Sum 50.0.\n",
      "Episode 460, Reward Sum 68.0.\n",
      "Episode 470, Reward Sum 34.0.\n",
      "Episode 480, Reward Sum 37.0.\n",
      "Episode 490, Reward Sum 52.0.\n",
      "Test reward: 62.4\n",
      "Episode 500, Reward Sum 38.0.\n",
      "Episode 510, Reward Sum 65.0.\n",
      "Episode 520, Reward Sum 83.0.\n",
      "Episode 530, Reward Sum 51.0.\n",
      "Episode 540, Reward Sum 62.0.\n",
      "Episode 550, Reward Sum 57.0.\n",
      "Episode 560, Reward Sum 80.0.\n",
      "Episode 570, Reward Sum 83.0.\n",
      "Episode 580, Reward Sum 62.0.\n",
      "Episode 590, Reward Sum 45.0.\n",
      "Test reward: 149.4\n",
      "Episode 600, Reward Sum 48.0.\n",
      "Episode 610, Reward Sum 32.0.\n",
      "Episode 620, Reward Sum 49.0.\n",
      "Episode 630, Reward Sum 58.0.\n",
      "Episode 640, Reward Sum 78.0.\n",
      "Episode 650, Reward Sum 37.0.\n",
      "Episode 660, Reward Sum 30.0.\n",
      "Episode 670, Reward Sum 148.0.\n",
      "Episode 680, Reward Sum 116.0.\n",
      "Episode 690, Reward Sum 50.0.\n",
      "Test reward: 144.2\n",
      "Episode 700, Reward Sum 68.0.\n",
      "Episode 710, Reward Sum 89.0.\n",
      "Episode 720, Reward Sum 54.0.\n",
      "Episode 730, Reward Sum 101.0.\n",
      "Episode 740, Reward Sum 54.0.\n",
      "Episode 750, Reward Sum 33.0.\n",
      "Episode 760, Reward Sum 45.0.\n",
      "Episode 770, Reward Sum 63.0.\n",
      "Episode 780, Reward Sum 80.0.\n",
      "Episode 790, Reward Sum 113.0.\n",
      "Test reward: 108.8\n",
      "Episode 800, Reward Sum 40.0.\n",
      "Episode 810, Reward Sum 63.0.\n",
      "Episode 820, Reward Sum 73.0.\n",
      "Episode 830, Reward Sum 71.0.\n",
      "Episode 840, Reward Sum 72.0.\n",
      "Episode 850, Reward Sum 37.0.\n",
      "Episode 860, Reward Sum 68.0.\n",
      "Episode 870, Reward Sum 82.0.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    global OBS_DIM\n",
    "    global ACTION_DIM\n",
    "\n",
    "    train_step_list = []\n",
    "    train_reward_list = []\n",
    "    evaluate_step_list = []\n",
    "    evaluate_reward_list = []\n",
    "\n",
    "    # 初始化游戏\n",
    "    env = gym.make('CartPole-v0')\n",
    "    # 图像输入形状和动作维度\n",
    "    action_dim = env.action_space.n\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    OBS_DIM = obs_dim\n",
    "    ACTION_DIM = action_dim\n",
    "    max_score = -int(1e4)\n",
    "\n",
    "    # 创建存储执行游戏的内存\n",
    "    MODEL = PolicyGradient(ACTION_DIM)\n",
    "    TARGET_MODEL = PolicyGradient(ACTION_DIM)\n",
    "\n",
    "    # 开始训练\n",
    "    print(\"start training...\")\n",
    "    # 训练max_episode个回合，test部分不计算入episode数量\n",
    "    for i in range(1000):\n",
    "        obs_list, action_list, reward_list = run_train(env, MODEL)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Episode {}, Reward Sum {}.\".format(i, sum(reward_list)))\n",
    "\n",
    "        batch_obs = np.array(obs_list)\n",
    "        batch_action = np.array(action_list)\n",
    "        batch_reward = calc_reward_to_go(reward_list)\n",
    "        cost = learn(batch_obs, batch_action, batch_reward, MODEL)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            total_reward = evaluate(MODEL, env, render=False) # render=True 查看渲染效果，需要在本地运行，AIStudio无法显示\n",
    "            print(\"Test reward: {}\".format(total_reward))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
