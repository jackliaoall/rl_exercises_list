{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8_1_以value_iteration方法實作平衡桿(cartpole).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNUCKLFj1lx1vYznhz6sjFH"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"npns1UfHV5Yi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614992205264,"user_tz":-480,"elapsed":17204,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"5f4c26d7-11c4-4d5d-d826-0ab707275933"},"source":["def value_iteration(env):\r\n","    v = np.zeros(100)\r\n","    a = np.zeros(100)\r\n","    for i_episode in range(10): #Episode次數\r\n","        observation = env.reset()\r\n","        rewards = 0\r\n","        for t in range(100): #t時間做回饋\r\n","            action = env.action_space.sample() #隨機選擇動作（只有左右）\r\n","            observation, reward, done, info = env.step(action) #回饋獎勵、狀態\r\n","            new_state = observation\r\n","            q_sa = sum([1/2*(reward + new_state)])\r\n","            v[t] = max(q_sa)\r\n","            a[t] = action\r\n","            rewards += reward\r\n","        \r\n","            if done:\r\n","                break\r\n","    env.close()\r\n","    return a,v,rewards\r\n","\r\n","def policy(env, iterations):\r\n","    p_temp = 0\r\n","    a_temp =[]\r\n","    p =[]\r\n","    for i in range(0,iterations):\r\n","        optimal_v = value_iteration(env)\r\n","        p = optimal_v[2]\r\n","        if p_temp < p:\r\n","            p_temp = p\r\n","            a_temp = optimal_v[0]\r\n","        else:\r\n","            pass\r\n","    return a_temp, p_temp\r\n","\r\n","import gym\r\n","import numpy as np\r\n","env = gym.make('CartPole-v0') #建立環境\r\n","a_temp, p_temp = policy(env,100)\r\n","a_temp, p_temp\r\n","\r\n","#CartPole-v0 \r\n","a_temp, p_temp = policy(env,3000)\r\n","a = a_temp.astype(int)\r\n","\r\n","def play(a):\r\n","\r\n","    for i_episode in range(0,10): #隨著每次環境分數不同總獎勵也會不同\r\n","        rewards = 0\r\n","        observation = env.reset()\r\n","        for t in range(0,len(a)):\r\n","            #print(a)\r\n","            #env.render()\r\n","            action = a[t]\r\n","            #print(action)\r\n","            observation, reward, done, info = env.step(action)\r\n","            #print(observation)\r\n","            rewards += reward\r\n","            if done:\r\n","                print(\"迭代結束 {} 時間步長\".format(t+1),\"總獎勵\", rewards)\r\n","                break\r\n","    env.close()\r\n","    return reward,p_temp\r\n","    \r\n","play(a)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["迭代結束 34 時間步長 總獎勵 34.0\n","迭代結束 39 時間步長 總獎勵 39.0\n","迭代結束 51 時間步長 總獎勵 51.0\n","迭代結束 37 時間步長 總獎勵 37.0\n","迭代結束 29 時間步長 總獎勵 29.0\n","迭代結束 45 時間步長 總獎勵 45.0\n","迭代結束 35 時間步長 總獎勵 35.0\n","迭代結束 44 時間步長 總獎勵 44.0\n","迭代結束 52 時間步長 總獎勵 52.0\n","迭代結束 75 時間步長 總獎勵 75.0\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(1.0, 100.0)"]},"metadata":{"tags":[]},"execution_count":2}]}]}